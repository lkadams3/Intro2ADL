{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab for Week 5: Fitting a Feed Forward Neural Network\n",
    "\n",
    "In Milestone 1 you chose a project topic you are interested in and found some datasets you could use. You also already built a [data loader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Today, we will fit a baseline feed forward neural network to one of the datasets you chose by following the pytorch tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a Data Loader\n",
    "You are encouraged to use one of the data loaders you built for milestone 1. Otherwise use the one described [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Tip: If your targets are labels, you might want to use the lambda transformation described [here](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html) to turn your target into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/45bde12ffb11ed63f138eb71f1686c49.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/9bc01747de8df68dd22eab215fc2d6ce.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/ac2e6a82fdda125099d8171730fb25b8.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f785ce9140e103f2df62cea456708aaa.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/8cfe54501fe00ddc74096b7cd67aa1e1.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/05ca8826b74e690961ffc4951777689f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/9d994f1278a9788612fa2af179328c31.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/aab39ed80019fefdabd07e7fa1125bc9.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/99226382ee26871f6f9f903eb928b6f3.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/d850993b213f1d50ad771dce91c5dcad.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/480fd80167d7865ce9aafe14aba4ec9d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/ffe7011351ab1c65eaeee1c3328bd239.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/ecacdf96f4a54f76834361a445194e0e.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/e502329e84cce457840d11ba086584c4.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/549f5533051f0a51b976135f28ef8a96.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/bb23fa10c71a57d17a8c7eb4773db237.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/65622d9d5904ab4b615cd3b754dddda2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/04379559d6f119c6028c6790c2b3af1c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/0f119ca91147595c6299c830803cfb47.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f0e1f501ec5f67e5ce9486a59304b4ab.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/a906e7bbb9307b70cb7223c67f39a41f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/4f3c20ff6ca4acbecc3d1639031c5cd2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/76190e02d6dbf53a4f6e2b2666fdbc63.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/0411ab5e3174cccbddc663b9dc97931c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/10e0293479132f664253bb7820dbb67a.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/8b53820171cc4ef6e17c56ec23c44e57.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/df159a481fad4726ac6d1abbe2529468.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/47e741a7840c2aa33f7c460fdad2fce2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/dc00687b3d681399aafef060c45826c2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/d653edb6aa702f095a9b00550738a821.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f2ddb6fd2d4348f5f562764849aebe12.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/2851ef5f2ee2e0035cf316eeaeac217b.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/202cbc4cbaf0dcc67fde7feb87362984.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/2ed97fd85d7c7f94523d60fa1e3c3dfc.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/280c6c1ec57789cd0f2ea50fb604762d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/fc63008d9d80b373d2d8f84dfd034446.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/48d8241a072b983a03bb30701adecb68.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/44bc5141b506eadc4e7233200d057125.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/7ee64c457bd9272561a39993e831460d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/6712381a925124e968bcac89481c90e0.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/b6cf008da895fb856a76c033fddf6ec5.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/033001225ab6fcdc1067b4a813484808.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/28126ac3ed45c0f1f282baa0c04232cd.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/4e0c618edaf786dcbd5c8b6a2807db9d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/267e230db9a29ddc9fb4c3b7a394a055.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/bb4fecc4701823a02f9c06f58e0da001.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/05175d29ac2b49702ba01bc4952cdb1c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f09ed213db3ecc825b7d66ea987828e9.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/3b713377bb9025c1ab6a184d788c940b.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/47b0958247b48d35ce3a91e907f6f299.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/87ae4c6a6e5d03d360fdd19b3a2d8092.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/93eaca74c2642ed15f1a6c58591bea3f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f71b43e686defe90d813f07c952dfe75.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/1f44599103695a126e5bb6496ba52cb7.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/85e11dd1c6f60d02f16516fdfac9c7f7.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/93997108c3ae3beacc108f8829dc2302.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/5b2233d744c8b8d46c8577b69ba82a8f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/acbc68bd9e35591b8344ab8b5e51f458.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/26c26cac0f8cb4b9ac269d2dd1bcb2e4.jpg\n",
      "13216\n",
      "3302\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize to 299x299, this is a better size for the type of images I'm using\n",
    "    transforms.ToTensor(),          # Converting images to tensor\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Loding the dataset\n",
    "fitzpatrick = 'https://raw.githubusercontent.com/mattgroh/fitzpatrick17k/refs/heads/main/fitzpatrick17k.csv'\n",
    "df = pd.read_csv(fitzpatrick)\n",
    "image_folder = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images'\n",
    "#Definnig the class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, image_folder):\n",
    "        self.image_folder = image_folder\n",
    "        # Filter out rows with missing images\n",
    "        valid_rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            image_path = os.path.join(image_folder, f\"{row['md5hash']}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                valid_rows.append(idx)\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {image_path}\")\n",
    "        self.data = df.loc[valid_rows].reset_index(drop=True)\n",
    "\n",
    "        self.features = self.data[['md5hash']].values\n",
    "        self.labels = self.data['label'].values\n",
    "\n",
    "        # Label encoding that will convert fitzpatrick string labels (skin tone classification) into integers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, f\"{self.features[idx][0]}.jpg\")\n",
    "    \n",
    "        if os.path.exists(image_path):\n",
    "            img = Image.open(image_path).convert(\"RGB\")  \n",
    "            img = transform(img)  # Apply transformations to the images (resize + ToTensor)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\") \n",
    "            #to check if the images are there or missing, because I was having issues with this originally\n",
    "    \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)  # To convert label to tensor\n",
    "    \n",
    "        return img, label  # To make sure the image is a tensor before returning, was getting errors from this \n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Making training and testing dataset!\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = CustomDataset(df_train, image_folder)\n",
    "test_dataset = CustomDataset(df_test, image_folder)\n",
    "\n",
    "print(len(train_dataset)) # just to make sure this worked\n",
    "print(len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used this cell to process the images, resize, and put them in a new folder\n",
    "\n",
    "# # Define paths\n",
    "# input_folder = \"/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images\"\n",
    "# output_folder = \"/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images\"\n",
    "\n",
    "# # Ensure output folder exists\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Define transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((299, 299))  # Resize to 299x299\n",
    "# ])\n",
    "\n",
    "# # Process images\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Add other formats if needed\n",
    "#         img_path = os.path.join(input_folder, filename)\n",
    "#         img = Image.open(img_path)\n",
    "#         img = transform(img)  # Apply resize transform\n",
    "#         img.save(os.path.join(output_folder, filename))  # Save resized image\n",
    "\n",
    "# print(\"Processing complete! All images are resized and saved in 'processed_images'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/45bde12ffb11ed63f138eb71f1686c49.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/9bc01747de8df68dd22eab215fc2d6ce.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/ac2e6a82fdda125099d8171730fb25b8.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f785ce9140e103f2df62cea456708aaa.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/8cfe54501fe00ddc74096b7cd67aa1e1.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/05ca8826b74e690961ffc4951777689f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/9d994f1278a9788612fa2af179328c31.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/aab39ed80019fefdabd07e7fa1125bc9.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/99226382ee26871f6f9f903eb928b6f3.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/d850993b213f1d50ad771dce91c5dcad.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/480fd80167d7865ce9aafe14aba4ec9d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/ffe7011351ab1c65eaeee1c3328bd239.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/ecacdf96f4a54f76834361a445194e0e.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/e502329e84cce457840d11ba086584c4.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/549f5533051f0a51b976135f28ef8a96.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/bb23fa10c71a57d17a8c7eb4773db237.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/65622d9d5904ab4b615cd3b754dddda2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/04379559d6f119c6028c6790c2b3af1c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/0f119ca91147595c6299c830803cfb47.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f0e1f501ec5f67e5ce9486a59304b4ab.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/a906e7bbb9307b70cb7223c67f39a41f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/4f3c20ff6ca4acbecc3d1639031c5cd2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/76190e02d6dbf53a4f6e2b2666fdbc63.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/0411ab5e3174cccbddc663b9dc97931c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/10e0293479132f664253bb7820dbb67a.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/8b53820171cc4ef6e17c56ec23c44e57.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/df159a481fad4726ac6d1abbe2529468.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/47e741a7840c2aa33f7c460fdad2fce2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/dc00687b3d681399aafef060c45826c2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/d653edb6aa702f095a9b00550738a821.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f2ddb6fd2d4348f5f562764849aebe12.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/2851ef5f2ee2e0035cf316eeaeac217b.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/202cbc4cbaf0dcc67fde7feb87362984.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/2ed97fd85d7c7f94523d60fa1e3c3dfc.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/280c6c1ec57789cd0f2ea50fb604762d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/fc63008d9d80b373d2d8f84dfd034446.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/48d8241a072b983a03bb30701adecb68.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/44bc5141b506eadc4e7233200d057125.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/7ee64c457bd9272561a39993e831460d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/6712381a925124e968bcac89481c90e0.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/b6cf008da895fb856a76c033fddf6ec5.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/033001225ab6fcdc1067b4a813484808.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/28126ac3ed45c0f1f282baa0c04232cd.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/4e0c618edaf786dcbd5c8b6a2807db9d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/267e230db9a29ddc9fb4c3b7a394a055.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/bb4fecc4701823a02f9c06f58e0da001.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/05175d29ac2b49702ba01bc4952cdb1c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f09ed213db3ecc825b7d66ea987828e9.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/3b713377bb9025c1ab6a184d788c940b.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/47b0958247b48d35ce3a91e907f6f299.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/87ae4c6a6e5d03d360fdd19b3a2d8092.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/93eaca74c2642ed15f1a6c58591bea3f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/f71b43e686defe90d813f07c952dfe75.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/1f44599103695a126e5bb6496ba52cb7.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/85e11dd1c6f60d02f16516fdfac9c7f7.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/93997108c3ae3beacc108f8829dc2302.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/5b2233d744c8b8d46c8577b69ba82a8f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/acbc68bd9e35591b8344ab8b5e51f458.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/26c26cac0f8cb4b9ac269d2dd1bcb2e4.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 299, 299]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#  DataLoader that I will use for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Example: Print the first batch\n",
    "for batch_X, batch_y in train_loader:\n",
    "    print(batch_X.shape, batch_y.shape)  \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from scipy.spatial import distance_matrix\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images will be saved to: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Original images folder\n",
    "working_path = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images'\n",
    "\n",
    "# New folder for processed images\n",
    "save_path = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images'\n",
    "\n",
    "# Create the new folder if it doesn't already exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Verify the folder was created\n",
    "print(f\"Processed images will be saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training dataset:\n",
      "13216\n",
      "length of test dataset:\n",
      "3302\n"
     ]
    }
   ],
   "source": [
    "print('length of training dataset:')\n",
    "print(len(train_dataset))\n",
    "print('length of test dataset:')\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4510, 0.3725, 0.3569,  ..., 0.5176, 0.5059, 0.4902],\n",
      "          [0.3333, 0.2941, 0.2824,  ..., 0.5333, 0.5176, 0.5137],\n",
      "          [0.3059, 0.3137, 0.2627,  ..., 0.5412, 0.5451, 0.5255],\n",
      "          ...,\n",
      "          [0.0314, 0.0784, 0.1059,  ..., 0.0078, 0.0039, 0.0039],\n",
      "          [0.0157, 0.0667, 0.1098,  ..., 0.0039, 0.0000, 0.0000],\n",
      "          [0.0431, 0.1020, 0.1412,  ..., 0.0039, 0.0039, 0.0039]],\n",
      "\n",
      "         [[0.2706, 0.2196, 0.2157,  ..., 0.2588, 0.2549, 0.2431],\n",
      "          [0.1804, 0.1451, 0.1490,  ..., 0.2784, 0.2745, 0.2706],\n",
      "          [0.1529, 0.1373, 0.1059,  ..., 0.2980, 0.3059, 0.2902],\n",
      "          ...,\n",
      "          [0.5294, 0.5255, 0.5294,  ..., 0.5922, 0.5843, 0.5843],\n",
      "          [0.5255, 0.5216, 0.5255,  ..., 0.5922, 0.5961, 0.5922],\n",
      "          [0.5176, 0.5216, 0.5294,  ..., 0.5882, 0.5922, 0.5882]],\n",
      "\n",
      "         [[0.2157, 0.1843, 0.1843,  ..., 0.1961, 0.1882, 0.1765],\n",
      "          [0.1451, 0.1176, 0.1216,  ..., 0.2157, 0.2039, 0.2000],\n",
      "          [0.1176, 0.0980, 0.0667,  ..., 0.2275, 0.2353, 0.2196],\n",
      "          ...,\n",
      "          [0.7529, 0.7490, 0.7451,  ..., 0.8353, 0.8275, 0.8314],\n",
      "          [0.7451, 0.7333, 0.7333,  ..., 0.8314, 0.8314, 0.8353],\n",
      "          [0.7451, 0.7412, 0.7412,  ..., 0.8275, 0.8314, 0.8353]]]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Model\n",
    "Follow [this tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) to define a model. Make sure that the input dimension fits the dimensionality of your data, and the output dimension fits the dimensionality of your targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(299 * 299 * 3, 512)  # Updated the input size to match the flattened image size\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 114)  # There are 114 output classes (skin types/diseases represented)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = torch.flatten(x, 1)  # Flatten the image\n",
    "\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output layer (no activation function, logits)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc1): Linear(in_features=268203, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=114, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Model\n",
    "\n",
    "Follow the tutorial [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters for optimization\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Runs one full training epoch on the given model using the provided dataloader.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of training data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be trained.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the model parameters.\n",
    "\n",
    "    Behavior:\n",
    "        - Sets the model to training mode (important for layers like dropout and batch normalization).\n",
    "        - Iterates over the data batches:\n",
    "            - Computes the model's predictions.\n",
    "            - Calculates the loss.\n",
    "            - Performs backpropagation and updates model weights.\n",
    "        - Prints progress every 100 batches, showing the current loss and the number of samples processed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "#    TODO: Implement this function\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of test data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "\n",
    "    Behavior:\n",
    "        - Sets the model to evaluation mode (which disables behaviors like dropout).\n",
    "        - Iterates over the test data without computing gradients (using torch.no_grad() for efficiency).\n",
    "        - Accumulates the total loss and counts the number of correct predictions.\n",
    "        - Computes the average loss and overall accuracy.\n",
    "        - Prints the test accuracy and average loss.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\" \n",
    "    # TODO: Implement this function\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x262144 and 1048576x114)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 24\u001b[0m     train_loop(train_loader, model, loss_fn, optimizer)\n\u001b[1;32m     25\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m test_loop(test_loader, model, loss_fn)\n\u001b[1;32m     27\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss)  \u001b[38;5;66;03m# Store the validation loss for each epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[148], line 32\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x262144 and 1048576x114)"
     ]
    }
   ],
   "source": [
    "# Train the model for 3 different hyper parameter settings (e.g. different learning rates, different loss functions that make sense for your data, etc.)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# epochs = 10\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")\n",
    "\n",
    "# Assuming `train_loop` and `test_loop` are defined to train and evaluate your model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5  # reduced to 5 because 10 was taking forever\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    val_loss = test_loop(test_loader, model, loss_fn)\n",
    "    \n",
    "    val_losses.append(val_loss)  # Wanted to store the validation loss so I'd be able to see it\n",
    "\n",
    "print(\"Validation losses over epochs:\", val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opimization\n",
    "\n",
    "# Define the model, loss function, and optimizer here (using your existing model)\n",
    "def create_model():\n",
    "    # Example: Define your model architecture (change this to your actual model)\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*128*128, 114)  # Change dimensions based on your input size\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter tuning\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    momentum = trial.suggest_uniform('momentum', 0.5, 0.9)\n",
    "    \n",
    "    # Recreate model and dataloaders for each trial\n",
    "    model = create_model()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Record results\n",
    "\n",
    "Results are typically recorded in a table. For inspiration, check out the famous [Attention is All You Need paper](https://arxiv.org/abs/1706.03762). This paper first introduced the transformer architecture we will learn about later this semseter and has been highly influential in deep learnin. Check out how results are reported in Tables 2, 3, and 4.\n",
    "Note that because here the transformers are used for text generation results are reported using the BLEU score (the higher the better). \n",
    "\n",
    "You should create your own result tables to record how different hyperparameter settings affect performance. Make sure to record test accuracy, not traingin accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
