{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import pytorch_lightning as pl\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from torchmetrics import __version__ as torchmetrics_version\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab for Week 5: Fitting a Feed Forward Neural Network\n",
    "\n",
    "In Milestone 1 you chose a project topic you are interested in and found some datasets you could use. You also already built a [data loader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Today, we will fit a baseline feed forward neural network to one of the datasets you chose by following the pytorch tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a Data Loader\n",
    "You are encouraged to use one of the data loaders you built for milestone 1. Otherwise use the one described [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "Tip: If your targets are labels, you might want to use the lambda transformation described [here](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html) to turn your target into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize to 299x299, this is a better size for the type of images I'm using\n",
    "    transforms.ToTensor(),          # Converting images to tensor\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Loding the dataset\n",
    "#fitzpatrick = 'https://raw.githubusercontent.com/mattgroh/fitzpatrick17k/refs/heads/main/fitzpatrick17k.csv'\n",
    "#df = pd.read_csv(fitzpatrick)\n",
    "# To this:\n",
    "fitzpatrick_url = 'https://raw.githubusercontent.com/mattgroh/fitzpatrick17k/refs/heads/main/fitzpatrick17k.csv'\n",
    "df = pd.read_csv(fitzpatrick_url)  # Correctly load the dataset from the URL\n",
    "\n",
    "image_folder = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images'\n",
    "#Definnig the class\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(3, 299, 299), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "        # new PL attributes:\n",
    "        if parse_version(torchmetrics_version) > parse_version(\"0.8\"):\n",
    "            self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.valid_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        else:\n",
    "            self.train_acc = Accuracy()\n",
    "            self.valid_acc = Accuracy()\n",
    "            self.test_acc = Accuracy()\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2] \n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units: \n",
    "            layer = nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(nn.ReLU()) \n",
    "            input_size = hidden_unit \n",
    " \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "        self.image_folder = image_folder\n",
    "        # Filter out rows with missing images\n",
    "        valid_rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            image_path = os.path.join(image_folder, f\"{row['md5hash']}.jpg\")\n",
    "            if os.path.exists(image_path):\n",
    "                valid_rows.append(idx)\n",
    "            else:\n",
    "                print(f\"Warning: Image not found: {image_path}\")\n",
    "        self.data = df.loc[valid_rows].reset_index(drop=True)\n",
    "\n",
    "        self.features = self.data[['md5hash']].values\n",
    "        self.labels = self.data['label'].values\n",
    "\n",
    "        # Label encoding that will convert fitzpatrick string labels (skin tone classification) into integers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_folder, f\"{self.features[idx][0]}.jpg\")\n",
    "    \n",
    "        if os.path.exists(image_path):\n",
    "            img = Image.open(image_path).convert(\"RGB\")  \n",
    "            img = transform(img)  # Apply transformations to the images (resize + ToTensor)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\") \n",
    "            #to check if the images are there or missing, because I was having issues with this originally\n",
    "    \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)  # To convert label to tensor\n",
    "    \n",
    "        return img, label  # To make sure the image is a tensor before returning, was getting errors from this \n",
    "# Conditionally define epoch end methods based on PyTorch Lightning version\n",
    "    if parse_version(pl.__version__) >= parse_version(\"2.0\"):\n",
    "        # For PyTorch Lightning 2.0 and above\n",
    "        def on_training_epoch_end(self):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def on_validation_epoch_end(self):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def on_test_epoch_end(self):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "\n",
    "    else:\n",
    "        # For PyTorch Lightning < 2.0\n",
    "        def training_epoch_end(self, outs):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def validation_epoch_end(self, outs):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def test_epoch_end(self, outs):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # Making training and testing dataset!\n",
    "# df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# train_dataset = CustomDataset(df_train, image_folder)\n",
    "# test_dataset = CustomDataset(df_test, image_folder)\n",
    "\n",
    "# print(len(train_dataset)) # just to make sure this worked\n",
    "# print(len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used this cell to process the images, resize, and put them in a new folder\n",
    "\n",
    "# # Define paths\n",
    "# input_folder = \"/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images\"\n",
    "# output_folder = \"/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images\"\n",
    "\n",
    "# # Ensure output folder exists\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Define transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((299, 299))  # Resize to 299x299\n",
    "# ])\n",
    "\n",
    "# # Process images\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Add other formats if needed\n",
    "#         img_path = os.path.join(input_folder, filename)\n",
    "#         img = Image.open(img_path)\n",
    "#         img = transform(img)  # Apply resize transform\n",
    "#         img.save(os.path.join(output_folder, filename))  # Save resized image\n",
    "\n",
    "# print(\"Processing complete! All images are resized and saved in 'processed_images'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106f00bb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  DataLoader that I will use for training and testing\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# # Example: Print the first batch\n",
    "# for batch_X, batch_y in train_loader:\n",
    "#     print(batch_X.shape, batch_y.shape)  \n",
    "#     break\n",
    "fitzpatrick = 'https://raw.githubusercontent.com/mattgroh/fitzpatrick17k/refs/heads/main/fitzpatrick17k.csv'\n",
    "df = pd.read_csv(fitzpatrick)\n",
    "image_folder = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images'\n",
    "#Definnig the class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path='./'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        fitzpatrick_df = pd.read_csv(self.data_path + \"/fitzpatrick.csv\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        # here note relevant\n",
    "        Fitzpatrick_all = Fitzpatrick( \n",
    "            root=self.data_path,\n",
    "            train=True,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "        self.train, self.val = random_split(\n",
    "            fitzpatrick_all, [55000, 5000], generator=torch.Generator().manual_seed(1)\n",
    "        )\n",
    "\n",
    "        self.test = Fitzpatrick( \n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)\n",
    "    \n",
    "    \n",
    "torch.manual_seed(1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/laureladams/anaconda3/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/laureladams/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /Users/laureladams/anaconda3/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/laureladams/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class FitzpatrickDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, image_folder, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # dataset = CustomDataset(self.df, self.image_folder, self.transform)\n",
    "        dataset = CustomDataset(self.df, self.image_folder)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train, self.val = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(1))\n",
    "        self.test = CustomDataset(self.df, self.image_folder, self.transform)  # Could split differently\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "# Usage\n",
    "data_module = FitzpatrickDataModule(df, image_folder, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106f00bb0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  DataLoader that I will use for training and testing\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# # Example: Print the first batch\n",
    "# for batch_X, batch_y in train_loader:\n",
    "#     print(batch_X.shape, batch_y.shape)\n",
    "#     break\n",
    "fitzpatrick = 'https://raw.githubusercontent.com/mattgroh/fitzpatrick17k/refs/heads/main/fitzpatrick17k.csv'\n",
    "df = pd.read_csv(fitzpatrick)\n",
    "image_folder = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images'\n",
    "#Definnig the class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path='./'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        fitzpatrick_df = pd.read_csv(self.data_path + \"/fitzpatrick.csv\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        # here note relevant\n",
    "        Fitzpatrick_all = Fitzpatrick(\n",
    "            root=self.data_path,\n",
    "            train=True,\n",
    "            transform=self.transform,\n",
    "            download=False\n",
    "        )\n",
    "\n",
    "        self.train, self.val = random_split(\n",
    "            fitzpatrick_all, [55000, 5000], generator=torch.Generator().manual_seed(1)\n",
    "        )\n",
    "\n",
    "        self.test = Fitzpatrick(\n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.transform,\n",
    "            download=False\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Users/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/fc63008d9d80b373d2d8f84dfd034446.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/480fd80167d7865ce9aafe14aba4ec9d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/acbc68bd9e35591b8344ab8b5e51f458.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/93997108c3ae3beacc108f8829dc2302.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/9d994f1278a9788612fa2af179328c31.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/6712381a925124e968bcac89481c90e0.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/bb23fa10c71a57d17a8c7eb4773db237.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/d653edb6aa702f095a9b00550738a821.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/8cfe54501fe00ddc74096b7cd67aa1e1.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/85e11dd1c6f60d02f16516fdfac9c7f7.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/5b2233d744c8b8d46c8577b69ba82a8f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/3b713377bb9025c1ab6a184d788c940b.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/28126ac3ed45c0f1f282baa0c04232cd.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/f0e1f501ec5f67e5ce9486a59304b4ab.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/267e230db9a29ddc9fb4c3b7a394a055.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/2ed97fd85d7c7f94523d60fa1e3c3dfc.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/47e741a7840c2aa33f7c460fdad2fce2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/2851ef5f2ee2e0035cf316eeaeac217b.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/aab39ed80019fefdabd07e7fa1125bc9.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/bb4fecc4701823a02f9c06f58e0da001.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/0f119ca91147595c6299c830803cfb47.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/04379559d6f119c6028c6790c2b3af1c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/ffe7011351ab1c65eaeee1c3328bd239.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/0411ab5e3174cccbddc663b9dc97931c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/44bc5141b506eadc4e7233200d057125.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/99226382ee26871f6f9f903eb928b6f3.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/48d8241a072b983a03bb30701adecb68.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/ac2e6a82fdda125099d8171730fb25b8.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/87ae4c6a6e5d03d360fdd19b3a2d8092.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/05ca8826b74e690961ffc4951777689f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/f09ed213db3ecc825b7d66ea987828e9.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/a906e7bbb9307b70cb7223c67f39a41f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/9bc01747de8df68dd22eab215fc2d6ce.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/f2ddb6fd2d4348f5f562764849aebe12.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/4f3c20ff6ca4acbecc3d1639031c5cd2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/202cbc4cbaf0dcc67fde7feb87362984.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/b6cf008da895fb856a76c033fddf6ec5.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/e502329e84cce457840d11ba086584c4.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/1f44599103695a126e5bb6496ba52cb7.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/76190e02d6dbf53a4f6e2b2666fdbc63.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/d850993b213f1d50ad771dce91c5dcad.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/ecacdf96f4a54f76834361a445194e0e.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/4e0c618edaf786dcbd5c8b6a2807db9d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/f71b43e686defe90d813f07c952dfe75.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/df159a481fad4726ac6d1abbe2529468.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/10e0293479132f664253bb7820dbb67a.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/45bde12ffb11ed63f138eb71f1686c49.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/26c26cac0f8cb4b9ac269d2dd1bcb2e4.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/033001225ab6fcdc1067b4a813484808.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/280c6c1ec57789cd0f2ea50fb604762d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/93eaca74c2642ed15f1a6c58591bea3f.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/65622d9d5904ab4b615cd3b754dddda2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/dc00687b3d681399aafef060c45826c2.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/47b0958247b48d35ce3a91e907f6f299.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/7ee64c457bd9272561a39993e831460d.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/8b53820171cc4ef6e17c56ec23c44e57.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/05175d29ac2b49702ba01bc4952cdb1c.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/549f5533051f0a51b976135f28ef8a96.jpg\n",
      "Warning: Image not found: /Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images/f785ce9140e103f2df62cea456708aaa.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laureladams/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CustomDataset.__init__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Fit the model using the DataModule\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model\u001b[38;5;241m=\u001b[39mfitzpatrickclassifier, datamodule\u001b[38;5;241m=\u001b[39mfitzpatrick_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    541\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:944\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    941\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: preparing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[0;32m--> 944\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_setup_hook(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[1;32m    945\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    946\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_configure_model(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:102\u001b[0m, in \u001b[0;36m_call_setup_hook\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     99\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_setup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mdatamodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     _call_lightning_datamodule_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, stage\u001b[38;5;241m=\u001b[39mfn)\n\u001b[1;32m    103\u001b[0m _call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, stage\u001b[38;5;241m=\u001b[39mfn)\n\u001b[1;32m    104\u001b[0m _call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup\u001b[39m\u001b[38;5;124m\"\u001b[39m, stage\u001b[38;5;241m=\u001b[39mfn)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:193\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mFitzpatrickDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# dataset = CustomDataset(self.df, self.image_folder, self.transform)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder)\n\u001b[1;32m     20\u001b[0m     train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     21\u001b[0m     val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n",
      "\u001b[0;31mTypeError\u001b[0m: CustomDataset.__init__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Initialize model\n",
    "fitzpatrickclassifier = MultiLayerPerceptron()\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")]  # save top 1 model\n",
    "\n",
    "# Initialize DataModule (using the image folder path defined earlier)\n",
    "fitzpatrick_data = FitzpatrickDataModule(df=df, image_folder='/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images')\n",
    "\n",
    "# Initialize Trainer\n",
    "if torch.cuda.is_available():  # if you have GPUs\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks, gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks)\n",
    "\n",
    "# Fit the model using the DataModule\n",
    "trainer.fit(model=fitzpatrickclassifier, datamodule=fitzpatrick_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging: check the type of fitzpatrick\n",
    "print(type(fitzpatrick))  # Check if it's a LightningDataModule or a string\n",
    "\n",
    "# Ensure that fitzpatrick is a LightningDataModule before passing it\n",
    "if not isinstance(fitzpatrick, pl.LightningDataModule):\n",
    "    raise TypeError(\"Expected a LightningDataModule, but got a string or other type. Check your data loading.\")\n",
    "\n",
    "# Now you can safely call trainer.fit\n",
    "trainer.fit(model=fitzpatrickclassifier, datamodule=fitzpatrick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that fitzpatrick is a LightningDataModule before passing it\n",
    "if isinstance(fitzpatrick, str):\n",
    "    raise TypeError(\"Expected a LightningDataModule, but got a string. Check your data loading.\")\n",
    "\n",
    "trainer.fit(model=fitzpatrickclassifier, datamodule=fitzpatrick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images/0a0e21f413499ad85018f7fa0df3efe2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from scipy.spatial import distance_matrix\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Original images folder\n",
    "working_path = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/images'\n",
    "\n",
    "# New folder for processed images\n",
    "save_path = '/Users/laureladams/Documents/School/spring2025/introtoADL/introToAppliedDeepLearning/processed_images'\n",
    "\n",
    "# Create the new folder if it doesn't already exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Verify the folder was created\n",
    "print(f\"Processed images will be saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of training dataset:')\n",
    "print(len(train_dataset))\n",
    "print('length of test dataset:')\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define a Model\n",
    "Follow [this tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) to define a model. Make sure that the input dimension fits the dimensionality of your data, and the output dimension fits the dimensionality of your targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "#         # Define the fully connected layers\n",
    "#         self.fc1 = nn.Linear(299 * 299 * 3, 512)  # Updated the input size to match the flattened image size\n",
    "#         self.fc2 = nn.Linear(512, 128)\n",
    "#         self.fc3 = nn.Linear(128, 114)  # There are 114 output classes (skin types/diseases represented)\n",
    "\n",
    "#     def forward(self, x):\n",
    "       \n",
    "#         x = torch.flatten(x, 1)  # Flatten the image\n",
    "\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)  # Output layer (no activation function, logits)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # new PL attributes:\n",
    "        if parse_version(torchmetrics_version) > parse_version(\"0.8\"):\n",
    "            self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.valid_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        else:\n",
    "            self.train_acc = Accuracy()\n",
    "            self.valid_acc = Accuracy()\n",
    "            self.test_acc = Accuracy()\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2] \n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units: \n",
    "            layer = nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(nn.ReLU()) \n",
    "            input_size = hidden_unit \n",
    " \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Conditionally define epoch end methods based on PyTorch Lightning version\n",
    "    if parse_version(pl.__version__) >= parse_version(\"2.0\"):\n",
    "        # For PyTorch Lightning 2.0 and above\n",
    "        def on_training_epoch_end(self):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def on_validation_epoch_end(self):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def on_test_epoch_end(self):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "\n",
    "    else:\n",
    "        # For PyTorch Lightning < 2.0\n",
    "        def training_epoch_end(self, outs):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def validation_epoch_end(self, outs):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def test_epoch_end(self, outs):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Model\n",
    "\n",
    "Follow the tutorial [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters for optimization\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Runs one full training epoch on the given model using the provided dataloader.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of training data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be trained.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used to update the model parameters.\n",
    "\n",
    "    Behavior:\n",
    "        - Sets the model to training mode (important for layers like dropout and batch normalization).\n",
    "        - Iterates over the data batches:\n",
    "            - Computes the model's predictions.\n",
    "            - Calculates the loss.\n",
    "            - Performs backpropagation and updates model weights.\n",
    "        - Prints progress every 100 batches, showing the current loss and the number of samples processed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "#    TODO: Implement this function\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a test dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing batches of test data (inputs and labels).\n",
    "        model (torch.nn.Module): The PyTorch model to be evaluated.\n",
    "        loss_fn (function): The loss function used to compute the error between predictions and true labels.\n",
    "\n",
    "    Behavior:\n",
    "        - Sets the model to evaluation mode (which disables behaviors like dropout).\n",
    "        - Iterates over the test data without computing gradients (using torch.no_grad() for efficiency).\n",
    "        - Accumulates the total loss and counts the number of correct predictions.\n",
    "        - Computes the average loss and overall accuracy.\n",
    "        - Prints the test accuracy and average loss.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\" \n",
    "    # TODO: Implement this function\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 3 different hyper parameter settings (e.g. different learning rates, different loss functions that make sense for your data, etc.)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# epochs = 10\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")\n",
    "\n",
    "# Assuming `train_loop` and `test_loop` are defined to train and evaluate your model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5  # reduced to 5 because 10 was taking forever\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    val_loss = test_loop(test_loader, model, loss_fn)\n",
    "    \n",
    "    val_losses.append(val_loss)  # Wanted to store the validation loss so I'd be able to see it\n",
    "\n",
    "print(\"Validation losses over epochs:\", val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opimization\n",
    "\n",
    "# Define the model, loss function, and optimizer here (using your existing model)\n",
    "def create_model():\n",
    "    # Example: Define your model architecture (change this to your actual model)\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*128*128, 114)  # Change dimensions based on your input size\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter tuning\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    momentum = trial.suggest_uniform('momentum', 0.5, 0.9)\n",
    "    \n",
    "    # Recreate model and dataloaders for each trial\n",
    "    model = create_model()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Record results\n",
    "\n",
    "Results are typically recorded in a table. For inspiration, check out the famous [Attention is All You Need paper](https://arxiv.org/abs/1706.03762). This paper first introduced the transformer architecture we will learn about later this semseter and has been highly influential in deep learnin. Check out how results are reported in Tables 2, 3, and 4.\n",
    "Note that because here the transformers are used for text generation results are reported using the BLEU score (the higher the better). \n",
    "\n",
    "You should create your own result tables to record how different hyperparameter settings affect performance. Make sure to record test accuracy, not traingin accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
